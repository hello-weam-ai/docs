---
title: "Simple Chat - Architecture and Flow"
description: "Understand the pipeline and execution model for Simple Chat integrating MCP tools and LangGraph-based control flow."
---

<Info>
  **Simple Chat now uses LangGraph and MCP integration for enhanced tool handling and modular flow**:
  - Dynamically pulls tools from MCP server
  - Builds a state machine using LangGraph to control execution between tools and chatbot
  - Supports new integrations: Slack, GitHub, Google Drive, Gmail, Google Calendar
</Info>

<Steps>

<Step title="User Query Flow">

1. User sends a query from the frontend UI.
2. The Python API receives the request and passes it to the **Service Controller**.
3. Controller uses the selected model (from dropdown) to decide which **Model Service** to initialize.
4. The selected LLM is initialized.
5. Simultaneously, a **MultiServerClient** fetches all available tools from the **MCP server**.

</Step>

<Step title="Tool Registration and Filtering">

- Tools received from MCP include:
  - Slack
  - GitHub
  - Google Drive
  - Google Calendar
  - Gmail
- These tools are matched against the current query.
- **Unnecessary tools are filtered out** before being **bound** with the LLM.

</Step>

<Step title="LangGraph StateGraph Construction">

- With tools filtered and bound, a **LangGraph StateGraph** is created:
  - Contains a `ToolNode` for tool execution.
  - Contains a `ChatbotNode` for normal LLM-based responses.

</Step>

<Step title="Chat History Initialization">

- In parallel, the **Chat Repository** is initialized using the provided `chat_id`.
- Full chat history is retrieved.
- History is used to **create memory** for the conversation flow.

</Step>

<Step title="StateGraph Execution">

- The LangGraph `StateGraph` is invoked.
- Based on the query and history:
  - If tool usage is detected → `ToolNode` is called
  - Else → `ChatbotNode` is used to generate a natural LLM response
- The combined context (query + history) is passed to the chatbot.

</Step>

<Step title="Tool Invocation Rules">

<Info>
  **Tool Activation Based on Model Selection**
</Info>

| Tool                 | Condition                                           |
|----------------------|-----------------------------------------------------|
| `Web Search Tool`    | Only used if **GPT-4.1-search** model is selected   |
| `Image Generation`   | Only used for **GET-based GPT models**             |
| `MCP Tools` (Slack, GitHub, etc.) | Dynamically fetched and filtered per query |

</Step>

<Step title="Streaming the Response">

1. The selected tool (if needed) is invoked.
2. The tool may internally call the LLM to complete its task.
3. The final response is streamed back to the **frontend in real-time**.

</Step>

<Step title="Logging and Cost Management">

- LLM token cost is calculated using input/output lengths.
- This cost is written to the DB using the **Cost Callback**.
- The **MongoDB Handler** logs:
  - Final response
  - Token usage
  - Tool activations
  - Total cost breakdown

</Step>

<Step title="Architecture Overview">

<img
  src="https://weam.ai/app/uploads/2025/07/simplechat.png"
  alt="LLM Query Flow Diagram"
  style={{ border: '1px solid #ccc', borderRadius: '8px', marginTop: '16px' }}
/>

</Step>

</Steps>

<Note>
  Web Search and Image Generation are **still model-gated** — verify model-specific logic before invoking or binding such tools. For deeper logic, inspect the Service Controller and LangGraph Node implementations.
</Note>

## Troubleshooting

<AccordionGroup>

<Accordion title="Tool does not trigger">

- Ensure the tool is compatible with the selected model.
- Verify MultiServerClient has pulled the latest tools from MCP.
- Check filtering logic — the tool might have been excluded during matching.
- Review LangGraph transition from ChatbotNode → ToolNode.

</Accordion>

<Accordion title="No streaming or delay in response">

- Confirm the frontend WebSocket/SSE connections are intact.
- Validate LangGraph state execution (especially transitions).
- Check if tool execution has bottlenecks or is calling LLM internally.
- Monitor MongoDB logs for signs of latency or cost overages.

</Accordion>

</AccordionGroup>
